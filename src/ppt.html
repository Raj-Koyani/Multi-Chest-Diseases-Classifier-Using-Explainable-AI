<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-Disease Chest X-ray Classifier | Raj Koyani</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;600;700&display=swap" rel="stylesheet">
    <style>
        /* CSS Reset and Global Styles */
        :root {
            --primary-color: #2E4636; /* Dark Green from image */
            --secondary-color: #f0f2f5; /* Light Gray background */
            --text-color-light: #F0EAD6; /* Off-white from image */
            --text-color-dark: #333;
            --card-bg: #ffffff;
            --shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
            --border-radius: 12px;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Poppins', sans-serif;
            background-color: var(--secondary-color);
            color: var(--text-color-dark);
            line-height: 1.7;
        }

        /* Main Container */
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }

        /* Header / Hero Section */
        .hero-header {
            background-color: var(--primary-color);
            color: var(--text-color-light);
            text-align: center;
            padding: 60px 20px;
            border-bottom-left-radius: var(--border-radius);
            border-bottom-right-radius: var(--border-radius);
            margin-bottom: 30px;
        }

        .hero-header h1 {
            font-size: 2.8rem;
            font-weight: 700;
            margin-bottom: 10px;
        }

        .hero-details {
            display: flex;
            justify-content: center;
            gap: 30px;
            margin-top: 20px;
            flex-wrap: wrap;
        }

        .hero-details p {
            font-size: 1.1rem;
            font-weight: 400;
        }
        
        .hero-details strong {
            font-weight: 600;
        }

        /* Card Styling */
        .card {
            background: var(--card-bg);
            border-radius: var(--border-radius);
            box-shadow: var(--shadow);
            padding: 30px;
            margin-bottom: 30px;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }

        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 8px 25px rgba(0, 0, 0, 0.12);
        }

        .card h2 {
            font-size: 1.8rem;
            color: var(--primary-color);
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--secondary-color);
        }
        
        .card h3 {
            font-size: 1.5rem;
            font-weight: 600;
            color: #333;
            margin-top: 25px;
            margin-bottom: 15px;
        }

        .card h4 {
            font-size: 1.1rem;
            font-weight: 600;
            margin-top: 15px;
            margin-bottom: 5px;
        }

        /* References Section Specific Styling */
        #references ol {
            padding-left: 20px;
        }
        #references li {
            margin-bottom: 15px;
            padding-left: 10px;
        }

        /* Approval Placeholders */
        .approvals-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
        }

        .approval-placeholder {
            border: 2px dashed #ccc;
            border-radius: var(--border-radius);
            padding: 20px;
            display: flex;
            align-items: center;
            justify-content: center;
            text-align: center;
            min-height: 200px;
            font-style: italic;
            color: #888;
            background-color: #fafafa;
        }

        /* Custom Lists Styling */
        .styled-list {
            list-style-type: none;
            padding-left: 0;
            margin-top: 10px;
        }
        
        .styled-list > li { /* Target only direct children of .styled-list */
            padding-left: 35px;
            position: relative;
            margin-bottom: 15px;
        }
        
        .styled-list > li strong { /* Target only direct children strong */
            color: var(--text-color-dark);
            font-weight: 600;
            display: block;
        }
        
        .styled-list > li::before { /* Target only direct children pseudo-elements */
            position: absolute;
            left: 0;
            top: 0;
            font-size: 1.4rem;
        }

        /* Nested lists */
        .styled-list ul {
            list-style-type: disc; /* Default disc for nested lists */
            padding-left: 20px;
            margin-top: 10px;
        }

        .styled-list ul li {
            padding-left: 10px; /* Adjust padding for nested items */
            margin-bottom: 8px;
        }

        .styled-list ul li::before {
            content: ''; /* Remove main icon from nested items */
        }
        
        /* Specific Icons for each section list */
        #problem-scope .styled-list li::before { content: 'âœ“'; color: var(--primary-color); font-weight: bold; }
        #research-challenges .styled-list li::before { content: 'âš ï¸'; }
        #research-objectives .styled-list li::before { content: 'ðŸŽ¯'; }
        #methodology .styled-list > li::before { content: 'âž¤'; color: var(--primary-color); } /* Only apply to direct children */

        /* New style for simple, clean lists like the results metrics */
        .results-list {
            list-style-type: none; /* Removes default bullets */
            padding-left: 10px; /* A little bit of indent */
            margin-top: 15px;
            margin-bottom: 25px;
        }

        .results-list li {
            margin-bottom: 10px; /* Space between items */
            font-size: 1.05rem; /* Slightly larger text */
        }

        .results-list li strong {
            font-weight: 600;
            color: var(--primary-color);
        }

        /* Literature Review Table Styling */
        .table-container {
            overflow-x: auto;
        }
        
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 20px 0;
            background: var(--card-bg);
        }

        th, td {
            border: 1px solid #e0e0e0;
            padding: 12px 15px;
            text-align: left;
        }

        th {
            background-color: var(--primary-color);
            color: white;
            font-weight: 600;
            text-align: center;
        }

        tr:nth-child(even) {
            background-color: #f9f9f9;
        }

        tr:hover {
            background-color: #e8f6f3;
        }

        td {
            font-size: 14px;
            text-align: center;
        }
        
        td:nth-child(2), td:nth-child(3) {
            text-align: left;
        }
        
        /* Footer */
        .footer {
            text-align: center;
            padding: 20px;
            margin-top: 20px;
            color: #777;
        }
        
        /* Responsive Design */
        @media (max-width: 768px) {
            .hero-header h1 {
                font-size: 2.2rem;
            }
            .hero-details {
                flex-direction: column;
                gap: 10px;
            }
            .card {
                padding: 20px;
            }
            .card h2 {
                font-size: 1.6rem;
            }
            .card h3 {
                font-size: 1.3rem;
            }
        }
    </style>
</head>
<body>

    <header class="hero-header">
        <h1>Multi-Disease Chest X-ray Classifier</h1>
        <div class="hero-details">
            <p><strong>Name:</strong> Raj Koyani</p>
            <p><strong>Reg no:</strong> 21MIS1017</p>
            <p><strong>Guide:</strong> Dr. Brindha V</p>
        </div>
    </header>

    <div class="container">

    <section class="card" id="approvals">
            <h2>Guide Approvals</h2>
            <div class="approvals-grid">
                <div class="approval-placeholder">
                    <img src="D:\pjt\image\ppt approval-1.png" alt="Guide's Approval for Student Review 1 PPT" style="max-width: 100%; height: auto; display: block; border-radius: 8px;">
                </div>
                <div class="approval-placeholder">
                    <img src="D:\pjt\image\vtop approval.png" alt="Guide Approval from VTOP" style="max-width: 100%; height: auto; display: block; border-radius: 8px;">
                </div>
            </div>
        </section>

        <section class="card" id="introduction">
            <h2>Introduction</h2>
            <p>The goal of this project is to develop a deep learning-based multi-disease chest X-ray classifier using <strong>DenseNet121</strong> and <strong>Swin-Transformer</strong>, integrated with <strong>GRAD-CAM</strong>. The system is designed to classify multiple respiratory diseases such as Viral Pneumonia, Tuberculosis (TB), COVID-19, and Lung Opacity from normal cases. Additionally, the model is equipped with Explainable AI (XAI) capabilities like Grad-CAM, which will help visualize the regions of the chest X-ray most relevant to the model's prediction, aiding clinical interpretation and supporting radiologists in diagnosis.</p>
        </section>
        
       <section class="card" id="literature-review">
    <h2>Literature Review</h2>
    <div class="table-container">
       <table>
  <thead>
    <tr>
      <th>S. No.</th>
      <th>Author(s), Year</th>
      <th>Method / Approach</th>
      <th>Key Contribution</th>
      <th>Gap Identified</th>
      <th>Accuracy</th>
      <th>AUROC</th>
      <th>Paper Link</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>Cid et al., 2024</td>
      <td>Deep neural networks (X-Raydar)</td>
      <td>Open-source AI for multi-label chest x-ray abnormality detection</td>
      <td>Requires clinical adoption</td>
      <td>94%</td>
      <td>0.953</td>
      <td><a href="https://www.thelancet.com/journals/landig/article/PIIS2589-7500(23)00218-2/fulltext" target="_blank">Link</a></td>
    </tr>
    <tr>
      <td>2</td>
      <td>Akhter et al., 2023</td>
      <td>AI/ML review on chest X-rays</td>
      <td>Structured review of CXR datasets, patents</td>
      <td>Need for larger dataset with segmentation masks</td>
      <td>93.45%</td>
      <td>0.9674</td>
      <td><a href="https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2023.1120989/full" target="_blank">Link</a></td>
    </tr>
    <tr>
      <td>3</td>
      <td>MirÃ³ Catalina et al., 2024</td>
      <td>AI CAD platform (ChestEye)</td>
      <td>Clinically validated AI for multi-class chest x-ray abnormalities</td>
      <td>More training required for certain conditions</td>
      <td>89.67%</td>
      <td>0.9345</td>
      <td><a href="https://www.nature.com/articles/s41598-024-55792-1" target="_blank">Link</a></td>
    </tr>
    <tr>
      <td>4</td>
      <td>Geric et al., 2023</td>
      <td>CAD software for TB</td>
      <td>Overview of AI CAD software for TB and thoracic diseases</td>
      <td>Regional variability in software performance</td>
      <td>92.78%</td>
      <td>0.9023</td>
      <td><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC10171486/" target="_blank">Link</a></td>
    </tr>
    <tr>
      <td>5</td>
      <td>Ã‡allÄ± et al., 2021</td>
      <td>Deep learning survey</td>
      <td>Survey of deep learning methods for chest X-ray analysis</td>
      <td>Need for explainability and clinical testing</td>
      <td>93.88%</td>
      <td>0.956</td>
      <td><a href="https://www.sciencedirect.com/science/article/pii/S1361841521001717" target="_blank">Link</a></td>
    </tr>
    <tr>
      <td>6</td>
      <td>Song et al., 2024</td>
      <td>AI chest X-ray enhancement</td>
      <td>Summary of image enhancement techniques aiding diagnosis</td>
      <td>Few integrated explainable AI methods</td>
      <td>96.78%</td>
      <td>0.978</td>
      <td><a href="https://www.sciencedirect.com/science/article/pii/S2666555724001205" target="_blank">Link</a></td>
    </tr>
    <tr>
      <td>7</td>
      <td>Naz et al., 2023</td>
      <td>ResNet50 + LIME</td>
      <td>Explainable AI for diseases like TB, pneumonia, COVID-19</td>
      <td>Wider dataset validation needed</td>
      <td>96%</td>
      <td>N/A</td>
      <td><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC9818469/" target="_blank">Link</a></td>
    </tr>
    <tr>
      <td>8</td>
      <td>Anderson et al., 2024</td>
      <td>Deep learning CAD system</td>
      <td>AI improves physician accuracy in chest abnormalities</td>
      <td>Need for more prospective clinical trials</td>
      <td>90.8%</td>
      <td>0.976</td>
      <td><a href="https://www.nature.com/articles/s41598-024-76608-2" target="_blank">Link</a></td>
    </tr>
    <tr>
      <td>9</td>
      <td>Kufel et al., 2023</td>
      <td>DenseNet121 + Grad-CAM</td>
      <td>Multi-label classification of 14 chest abnormalities</td>
      <td>Imbalanced data and explainability issues</td>
      <td>82.6%</td>
      <td>0.896</td>
      <td><a href="https://arxiv.org/html/2202.03583v4" target="_blank">Link</a></td>
    </tr>
    <tr>
      <td>10</td>
      <td>Mahamud et al., 2024</td>
      <td>DenseNet201 + SHAP, LIME</td>
      <td>Multi-disease classification with 99.2% accuracy</td>
      <td>Explainability validation needed</td>
      <td>94.2%</td>
      <td>0.965</td>
      <td><a href="https://www.sciencedirect.com/science/article/pii/S2772662224001036" target="_blank">Link</a></td>
    </tr>
    <tr>
      <td>11</td>
      <td>Fu et al., 2025</td>
      <td>LungMaxViT hybrid transformer</td>
      <td>Hybrid CNN+Transformer model with explainable heatmaps</td>
      <td>Clinical testing needed</td>
      <td>96.8%</td>
      <td>0.932</td>
      <td><a href="https://www.nature.com/articles/s41598-025-90607-x" target="_blank">Link</a></td>
    </tr>
    <tr>
      <td>12</td>
      <td>Shah et al., 2024</td>
      <td>Ensemble + transfer learning</td>
      <td>Review on datasets</td>
      <td>Need for standardized benchmarking</td>
      <td>91.89%</td>
      <td>0.948</td>
      <td><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11446784/" target="_blank">Link</a></td>
    </tr>
    <tr>
      <td>13</td>
      <td>Koyyada et al., 2023</td>
      <td>Weakly supervised CNN</td>
      <td>Localized disease identification mimicking radiologists</td>
      <td>XAI for multi-label cases required</td>
      <td>94.67%</td>
      <td>0.967</td>
      <td><a href="https://www.sciencedirect.com/science/article/pii/S2772442523000734" target="_blank">Link</a></td>
    </tr>
    <tr>
      <td>14</td>
      <td>Bhusal & Panday, 2023</td>
      <td>DenseNet121 + Grad-CAM</td>
      <td>Multi-label thoracic disease prediction</td>
      <td>Rare disease detection improvement needed</td>
      <td>82.6%</td>
      <td>0.896</td>
      <td><a href="https://arxiv.org/html/2202.03583v4" target="_blank">Link</a></td>
    </tr>
    <tr>
      <td>15</td>
      <td>Cervantes et al., 2024</td>
      <td>CNN transfer learning</td>
      <td>DenseNet201 best multi-label accuracy 96.47%</td>
      <td>Lacking external validation</td>
      <td>92.47%</td>
      <td>0.977</td>
      <td><a href="https://arxiv.org/html/2505.16028v1" target="_blank">Link</a></td>
    </tr>
    <tr>
      <td>16</td>
      <td>Ihongbe et al., 2024</td>
      <td>XAI systems evaluation</td>
      <td>Review of explainable AI techniques for chest radiographs</td>
      <td>Clinical adoption challenges</td>
      <td>88.90%</td>
      <td>0.934</td>
      <td><a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0308758" target="_blank">Link</a></td>
    </tr>
    <tr>
      <td>17</td>
      <td>Ong et al., 2021</td>
      <td>SqueezeNet + LIME, SHAP</td>
      <td>Multi-class including COVID-19 with explainability</td>
      <td>Lower accuracy than newer models</td>
      <td>84.3%</td>
      <td>N/A</td>
      <td><a href="https://arxiv.org/html/2505.16028v1" target="_blank">Link</a></td>
    </tr>
    <tr>
      <td>18</td>
      <td>Huang et al., 2024</td>
      <td>Multi-disease CNN diagnosis</td>
      <td>Improved detection using ChestX-ray14</td>
      <td>Limited external validation</td>
      <td>95.78%</td>
      <td>0.967</td>
      <td><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC10963853/" target="_blank">Link</a></td>
    </tr>
    <tr>
      <td>19</td>
      <td>Al-Adhaileh et al., 2025</td>
      <td>MobileNetV2</td>
      <td>92% accuracy on institutional dataset</td>
      <td>Generalizability beyond institution needed</td>
      <td>92%</td>
      <td>N/A</td>
      <td><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11926135/" target="_blank">Link</a></td>
    </tr>
    <tr>
      <td>20</td>
      <td>Gasca Cervantes et al., 2024</td>
      <td>VGG16, DenseNet201, ResNet50</td>
      <td>DenseNet201 highest accuracy 96.47%</td>
      <td>Improved interpretability needed</td>
      <td>96.47%</td>
      <td>N/A</td>
      <td><a href="https://arxiv.org/html/2505.16028v1" target="_blank">Link</a></td>
    </tr>
    <tr>
      <td>21</td>
      <td>de Camargo et al., 2025</td>
      <td>CNN (multi-model)</td>
      <td>Clinically validated AI algorithm for tuberculosis and CXR findings</td>
      <td>Agreement/impact on physician decisions remains low</td>
      <td>91.3%</td>
      <td>0.94</td>
      <td><a href="https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2025.1512910/full" target="_blank">Link</a></td>
    </tr>
    <tr>
      <td>22</td>
      <td>Konica Minolta R&D, 2025</td>
      <td>Deep learning + proprietary pre/post-processing (Finding-i)</td>
      <td>Commercial diagnostic support, improves reader accuracy</td>
      <td>Need for improved specificity and multi-condition coverage</td>
      <td>89%</td>
      <td>0.878</td>
      <td><a href="https://research.konicaminolta.com/en/report/2025/technical-papers/introduction-to-cxr-finding-i-an-ai-powered-chest-x-ray-diagnostic-support-system/" target="_blank">Link</a></td>
    </tr>
    <tr>
      <td>23</td>
      <td>Bhave et al., 2024</td>
      <td>Deep learning ensemble</td>
      <td>Detect left ventricular abnormalities from CXR; open dataset</td>
      <td>AUROC lower than standard CXR multi-disease AI</td>
      <td>71-80% sensitivity</td>
      <td>0.80</td>
      <td><a href="https://academic.oup.com/eurheartj/article/45/22/2002/7617354" target="_blank">Link</a></td>
    </tr>
    <tr>
      <td>24</td>
      <td>Monti et al., 2025</td>
      <td>AI system for Pneumothorax detection</td>
      <td>Diagnostic performance on PTX in CXR vs. radiologists</td>
      <td>Variable performance across institutions</td>
      <td>92.1%</td>
      <td>0.91</td>
      <td><a href="https://www.sciencedirect.com/science/article/pii/S0899707124002857" target="_blank">Link</a></td>
    </tr>
    <tr>
      <td>25</td>
      <td>Wienholt et al., 2025</td>
      <td>MedicalPatchNet (Patch-based Self-Explainable AI)</td>
      <td>Intrinsic interpretability, multi-disease CXR classification</td>
      <td>Further clinical benchmarking needed</td>
      <td>88.7%</td>
      <td>0.907</td>
      <td><a href="https://arxiv.org/abs/2509.07477" target="_blank">Link</a></td>
    </tr>
  </tbody>
</table>
    </div>
</section>

       <section class="card" id="datasets">
    <h2>Datasets Used</h2>

    <p>This project uses four primary data sources (plus ImageNet for pretraining). The table below lists each source, origin, counts used in the study, and the key role of each set.</p>

   <table class="data-table" summary="Dataset sources, counts, and roles">
    <thead>
        <tr>
            <th>Dataset</th>
            <th>Source</th>
            <th>Counts (used)</th>
            <th>Key Role</th>
        </tr>
    </thead>
    <tbody>  
        <tr>
            <td>Tuberculosis (TB) Chest X-ray Dataset</td>
            <td>
                <a href="https://www.kaggle.com/datasets/tawsifurrahman/tuberculosis-tb-chest-xray-dataset" target="_blank">
                    Kaggle
                </a>
            </td>
            <td>700 TB + 3,500 Normal chest</td>
            <td>Provides TB-positive examples and additional normal chest images for class balance and negative samples.</td>
        </tr>

        <tr>
            <td>COVID-19 Radiography Database</td>
            <td>
                <a href="https://www.kaggle.com/datasets/tawsifurrahman/covid19-radiography-database" target="_blank">
                    Kaggle
                </a>
            </td>
            <td>3,616 COVID-19 + 6,012 Lung Opacity + 1,345 Viral Pneumonia + ~3,000 Normal</td>
            <td>Primary source for COVID, viral pneumonia, and lung opacity classes; large diversity of pneumonia-like presentations.</td>
        </tr>

        <tr>
            <td>X-Ray Body Part Dataset (512Ã—512)</td>
            <td>
                <a href="https://www.kaggle.com/datasets/yovinyahathugoda/x-ray-body-part-dataset-512x512?utm_source=chatgpt.com" target="_blank">
                    Kaggle
                </a>
            </td>
            <td>4,876 Non-Chest images</td>
            <td>Non-chest images used as noise/negative samples to improve the model's robustness to non-diagnostic inputs.</td>
        </tr>

        <tr>
            <td>ImageNet-1K (pretraining)</td>
            <td>
                <a href="https://www.kaggle.com/datasets/sautkin/imagenet1k1?utm_source=chatgpt.com" target="_blank">
                    Kaggle / public
                </a>
            </td>
            <td>~1.2 million (used for pretraining only)</td>
            <td>Backbone pretraining to provide strong image-level feature representations before fine-tuning on X-ray data.</td>
        </tr>
    </tbody>
</table>


    <p class="summary"><strong>Disease-class breakdown (counts used in training):</strong></p>
    <ul class="styled-list">
        <li>Lung Opacity: 6,012 â€” largest pathology class, often indicates fluid/tissue changes associated with pneumonia.</li>
        <li>COVID-19: 3,616 â€” crucial to detect for timely clinical intervention.</li>
        <li>Viral Pneumonia: 1,345 â€” ensures the model can distinguish COVID-19 from other viral infections.</li>
        <li>Tuberculosis (TB): 700 â€” important to identify this distinct bacterial infection.</li>
        <li>Non-Chest Images (noise): 4,876 â€” used to improve negative-case robustness and reduce false positives.</li>
    </ul>

    <p class="note"><strong>Total images used for training (disease + non-chest): â‰ˆ 16,549</strong>. (ImageNet is used only for backbone pretraining and is not counted in the fine-tuning training total.)</p>
</section>

<!-- Problem Statement & Scope (updated content only) -->
<section class="card" id="problem-scope">
    <h2>Problem Statement &amp; Scope</h2>

    <h3>Problem Statement</h3>
    <p>
        Respiratory diseases such as COVID-19, Viral Pneumonia, Tuberculosis (TB), and conditions presenting as Lung Opacity are major contributors to global morbidity and mortality.
        Manual interpretation of chest X-rays is time-consuming and subject to inter- and intra-observer variability. While deep learning models can reach high accuracy,
        their opaque decision-making ("black-box") hinders clinical trust and adoption. Therefore, there is a pressing need for an <strong>explainable AI (XAI) based multi-disease classifier</strong>
        that delivers accurate predictions together with clear, human-interpretable explanations for those predictions.
    </p>

    <h3>Scope of the Project</h3>
    <ul class="styled-list">
        <li><strong>Data Utilization:</strong> Fine-tuning and evaluation are performed on a curated training set of â‰ˆ16,549 images spanning Lung Opacity, COVID-19, Viral Pneumonia, Tuberculosis (TB), and Non-Chest images used as negative/noise samples. ImageNet-1K is used for backbone pretraining (transfer learning).</li>
        <li><strong>Model Development:</strong> Combining DenseNet121 with Swin-Transformer components to capture both local radiographic patterns and global contextual features for robust multi-class classification.</li>
        <li><strong>Explainable AI Integration:</strong> Implementing XAI methods such as Grad-CAM (and complementary attribution techniques) to produce heatmaps and saliency maps that explain model predictions in radiological terms.</li>
        <li><strong>Evaluation &amp; Validation:</strong> Performance will be assessed using accuracy, precision, recall, F1-score, confusion matrices, and AUC-ROC for each class. Robustness checks include testing with non-chest images and class-imbalance experiments.</li>
        <li><strong>Practical Relevance:</strong> Provide interpretable outputs to assist radiologists, reduce reading time and diagnostic errors, and increase trust in AI-assisted radiology workflows through transparent visual explanations.</li>
    </ul>
</section>

        <section class="card" id="research-challenges">
    <h2>Research Challenges</h2>
    <ul class="styled-list">
        <li><strong>Data Imbalance:</strong> Uneven class distributionâ€”especially fewer TB and Viral Pneumonia samplesâ€”can bias the model. Techniques like augmentation and class weighting are used to balance training.</li>
        
        <li><strong>Image Quality Variability:</strong> Chest X-rays vary in contrast, resolution, and lighting due to different machines and acquisition settings. Standardization through preprocessing (resizing to 256Ã—256, grayscale conversion, histogram equalization) is essential.</li>
        
        <li><strong>Limited Data for Rare Diseases:</strong> Scarcity of labeled medical data for TB and Viral Pneumonia increases overfitting risks. Transfer learning and data augmentation mitigate this issue.</li>
        
        <li><strong>Overlapping Radiological Features:</strong> Diseases like COVID-19 and Viral Pneumonia often appear visually similar. Attention-based modules (CBAM, Swin-Transformer) help focus on discriminative regions.</li>
        
        <li><strong>Interpretability & Trust:</strong> Deep learning models must be explainable to gain clinical trust. Grad-CAM and SHAP provide visual insights into model decisions, though consistent clinical relevance remains challenging.</li>
        
        <li><strong>High Computational Demand:</strong> DenseNet121 + Swin-Transformer architectures require powerful GPUs and long training times. Optimization is vital for real-time applications.</li>
        
        <li><strong>Generalization:</strong> Ensuring consistent performance across hospitals and imaging setups demands robust preprocessing, cross-validation, and domain adaptation techniques.</li>
        
        <li><strong>Ethical & Privacy Constraints:</strong> Handling medical data must comply with HIPAA/GDPR regulations and anonymization standards, adding complexity to dataset preparation.</li>
        
        <li><strong>Deployment & Usability:</strong> Building a lightweight, user-friendly interface (Streamlit app) that delivers fast, interpretable predictions while maintaining accuracy across devices is a key implementation challenge.</li>
    </ul>
</section>


        
        
        <section class="card" id="methodology">
            <h2>Proposed Architecture and Methodology</h2>

            <h3>1. Data Collection and Preprocessing</h3>
            <p>A consolidated dataset of approximately <strong>16,549 chest X-ray images</strong> was constructed by merging three publicly available datasets, encompassing five major classesâ€”COVID-19, Lung Opacity, Viral Pneumonia, Tuberculosis (TB), and Non-Chest (noise) images. 
        Additionally, the <strong>ImageNet-1K</strong> dataset was used for backbone pretraining to enhance feature learning through transfer learning.</p>
            <ul class="styled-list">
                <li><strong>Datasets Used:</strong>
                    <ul>
                         <li>Tuberculosis (TB) Chest X-ray Dataset (Kaggle)</li>
                         <li>COVID-19 Radiography Database (Kaggle)</li>
                        <li>X-Ray Body Part Dataset (512Ã—512) (Kaggle)</li>
                        <li>ImageNet-1K (kaggle)</li>
                    </ul>
                </li>
                 <li><strong>Image Count Summary (after cleaning and merging):</strong>
            <ul>
                <li>Lung Opacity: 6,012 images</li>
                <li>COVID-19: 3,616 images</li>
                <li>Viral Pneumonia: 1,345 images</li>
                <li>Tuberculosis (TB): 700 images</li>
                <li>Non-Chest Images (noise): 4,876 images</li>
                <li><strong>Total (training): â‰ˆ16,549 images</li>
            </ul>
        </li>
                 <li><strong>Preprocessing Steps:</strong>
                    <ul>
                        <li>Scans all disease folders.</li>
                        <li>Convert â†’Grayscale.</li>
                        <li>Pad â†’Square aspect ratio.</li>
                        <li>Resize â†’256Ã—256 px.</li>
                        <li>Apply Histogram Equalization for contrast normalization.</li>
                        <li>Save preprocessed images to a new directory while maintaining folder structure.</li>
                        <li>Generate a manifest CSV with metadata (filename, class, resolution, status).</li>
                        <li>Automatically skip corrupted and duplicate images.</li>
                    </ul>
                </li>
            </ul>

              <h3>2. Model Architecture</h3>
        <p>The proposed model integrates DenseNet121 with a Convolutional Block Attention Module (CBAM):</p>
        <ul class="styled-list">
            <li><strong>Backbone:</strong> DenseNet121 pretrained on ImageNet for feature extraction.</li>
            <li><strong>Attention Module:</strong> CBAM to refine spatial and channel-wise attention, emphasizing radiologically significant regions.</li>
        </ul>

        <div style="text-align: center; margin-top: 25px;">
            <img 
                src="D:\pjt\image\archi.png" alt="Diagram of DenseNet121 architecture with a CBAM module" 
                style="max-width: 85%; height: auto; display: block; margin-left: auto; margin-right: auto; border-radius: var(--border-radius); box-shadow: var(--shadow);">
            <p style="font-size: 0.9rem; color: #777; margin-top: 10px;">
                <em>Fig. 1: Visualization of the DenseNet backbone with an integrated CBAM attention module, which sequentially refines features through channel and spatial attention.</em>
            </p>
        </div>

          <h3>3. Training Strategy</h3>
<ul class="styled-list">
    <li><strong>Dataset:</strong> The final dataset comprises approximately 16,549 chest X-ray images across five categories â€” COVID-19, Lung Opacity, Viral Pneumonia, Tuberculosis (TB), and Non-Chest (noise) images.</li>
    
    <li><strong>Input Preprocessing:</strong> All images are converted to grayscale, contrast-normalized using histogram equalization, and resized to 256Ã—256 px before training.</li>
    
    <li><strong>Hardware:</strong> Training is conducted on CPU-enabled systems but it can be improved by using GPU-enabled systems (NVIDIA RTX series) to accelerate computation and model convergence.</li>
    
    
    <li><strong>Batch Size & Epochs:</strong> Training is performed with a batch size of 32 for approximately 20â€“30 epochs, depending on early stopping criteria.</li>
    
    <li><strong>Data Split & Validation:</strong> The dataset is divided into 75% training, 10% validation, and 15% testing. A 5-fold cross-validation approach ensures robust generalization and reliability of results.</li>
    
    <li><strong>Performance Metrics:</strong> Evaluation includes accuracy, precision, recall, F1-score, and AUC-ROC. The target is to achieve â‰¥95% accuracy and â‰¥0.97 AUROC across all disease classes.</li>
</ul>


            <h3>4. Evaluation Metrics</h3>
<ul class="styled-list">
    <li><strong>Accuracy:</strong> The proposed model achieved an overall classification accuracy of 99% on the test dataset (420 images), demonstrating excellent generalization.</li>
    
    <li><strong>Per-Class Performance:</strong></li>
    <table class="data-table" summary="Per-class performance metrics">
        <thead>
            <tr>
                <th>Class</th>
                <th>Precision</th>
                <th>Recall</th>
                <th>F1-Score</th>
                <th>Support</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>COVID</td>
                <td>0.99</td>
                <td>0.97</td>
                <td>0.98</td>
                <td>97</td>
            </tr>
            <tr>
                <td>Lung Opacity</td>
                <td>0.98</td>
                <td>0.99</td>
                <td>0.99</td>
                <td>107</td>
            </tr>
            <tr>
                <td>Tuberculosis</td>
                <td>0.99</td>
                <td>1.00</td>
                <td>1.00</td>
                <td>110</td>
            </tr>
            <tr>
                <td>Viral Pneumonia</td>
                <td>1.00</td>
                <td>1.00</td>
                <td>1.00</td>
                <td>106</td>
            </tr>
        </tbody>
    </table>

    <li><strong>Macro Average:</strong> Precision = 0.99, Recall = 0.99, F1-Score =0.99</li>
    <li><strong>Weighted Average:</strong> Precision = 0.99, Recall = 0.99, F1-Score = 0.99</li>
    
    <li><strong>AUROC:</strong> The model achieved a mean AUC-ROC of approximately 0.99, indicating excellent class separability.<br>
        <img src="D:\pjt\image\evaluation report.png" alt="Confusion Matrix" style="margin-top:10px; border-radius:10px; box-shadow:0 2px 6px rgba(0,0,0,0.2); max-width:600px; display:block;">
    </li>
    
    <li><strong>Confusion Matrix:</strong> Shows near-perfect diagonal dominance, confirming high confidence and minimal misclassification among all disease classes.<br>
        <img src="D:\pjt\image\confusion mat.png" alt="Confusion Matrix" style="margin-top:10px; border-radius:10px; box-shadow:0 2px 6px rgba(0,0,0,0.2); max-width:600px; display:block;">
    </li>
    
     
</ul>


            <h3>5. Explainable AI (XAI) Integration</h3>
            <p>To ensure model interpretability, the system integrates Grad-CAM and SHAP:</p>
            <ul class="styled-list">
                <li><strong>Grad-CAM:</strong> Generates heatmaps highlighting suspicious lung regions.</li>
                <li><strong>Outputs:</strong> Original X-ray + predicted probabilities + visual explanations side-by-side.</li>
            </ul>

            <h3>6. Deployment Methodology</h3>
            <p>A Streamlit web application was developed with:</p>
            <ul class="styled-list">
                <li>User-friendly image upload interface.</li>
                <li>Real-time classification across 5 categories.</li>
                <li>Display of prediction confidence, Grad-CAM heatmaps.</li>
                <li>GPU-based backend for fast inference.</li>
                <li>Clinician-friendly design requiring minimal technical expertise.</li>
            </ul>
        </section>

      <section class="card" id="results-discussion">
    <h2>Results & Discussion</h2>

    <h3>1. Final Performance Metrics</h3>
    <p>
        The final modelâ€”based on the hybrid <strong>DenseNet121 + Swin-Transformer</strong> architectureâ€”was trained on a curated dataset of <strong>16,549 chest X-ray images</strong> 
        across four disease classes (COVID-19, Lung Opacity, Tuberculosis, Viral Pneumonia) and one Non-Chest (noise) class. 
        The model converged after approximately <strong>50 epochs</strong> with stable validation loss and near-perfect performance on the test set.
    </p>

    <ul class="results-list">
        <li>
            <strong>Test Accuracy:</strong> <strong>99.0%</strong> 
            <em>(Overall percentage of correct predictions)</em>
        </li>
        <li>
            <strong>Macro F1-Score:</strong> <strong>0.99</strong> 
            <em>(Balanced performance across all disease classes)</em>
        </li>
        <li>
            <strong>Macro Precision:</strong> <strong>0.99</strong>, 
            <strong>Macro Recall:</strong> <strong>0.99</strong>
            <em>(Demonstrating exceptional sensitivity and specificity)</em>
        </li>
        <li>
            <strong>AUROC:</strong> <strong>0.99</strong> 
            <em>(Excellent discrimination between disease categories)</em>
        </li>
    </ul>

    <h3>2. Discussion of Results</h3>
    <p>
        The proposed hybrid model demonstrated outstanding performance in multi-disease classification of chest X-rays, 
        achieving high precision, recall, and F1-scores across all classes. The integration of transformer-based global context learning 
        with DenseNetâ€™s local feature extraction allowed the model to capture both fine-grained and high-level visual cues.
    </p>

    <h4>Model Convergence</h4>
    <p>
        The model exhibited rapid convergence within the first 10 epochs and maintained stable loss and accuracy values throughout training. 
        The use of <strong>AdamW optimizer</strong> and a <strong>cosine annealing learning rate scheduler</strong> ensured smooth optimization and prevented overfitting. 
        Validation accuracy closely tracked training accuracy, confirming strong generalization.
    </p>

    <h4>Impact of Explainable AI (Grad-CAM)</h4>
    <p>
        The integration of <strong>Grad-CAM</strong> provided clear heatmaps highlighting the regions of the lungs contributing most to the modelâ€™s predictions. 
        This not only improved interpretability but also validated that the model focuses on clinically relevant anatomical regions, 
        enhancing trust in AI-assisted diagnosis.
    </p>

    <h4>Model Robustness and Generalization</h4>
    <p>
        Despite variations in image sources and quality, the model maintained consistent performance across all disease categories. 
        The inclusion of Non-Chest images improved robustness by helping the network distinguish between diagnostic and irrelevant inputs. 
        Cross-validation further confirmed model stability and reliability.
    </p>

    <h4>Strengths & Limitations</h4>
    <ul class="styled-list">
        <li><strong>Strengths:</strong>
            <ul>
                <li><strong>Exceptional Performance:</strong> 99% accuracy and F1-score across multiple diseases.</li>
                <li><strong>Strong Interpretability:</strong> Grad-CAM visualizations confirm that the modelâ€™s decisions align with clinical reasoning.</li>
                <li><strong>Robust Generalization:</strong> Performs well on unseen validation data with minimal class confusion.</li>
            </ul>
        </li>
        <li><strong>Limitations:</strong>
            <ul>
                <li><strong>Computational Demand:</strong> DenseNet121 + Swin-Transformer architecture requires high GPU memory and long training times (â‰ˆ1â€“1.5 hours per epoch).</li>
                <li><strong>Dataset Size:</strong> Some classes, particularly Tuberculosis and Viral Pneumonia, have limited data, suggesting potential for further augmentation or synthetic data generation.</li>
            </ul>
        </li>
    </ul>

    <h4>Confusion Matrix</h4>
    <p>
        The confusion matrix shows near-perfect diagonal dominance, indicating almost no misclassification between disease categories.
    </p>
    <img src="D:\pjt\image\confusion mat.png" alt="Confusion Matrix" style="margin-top:10px; border-radius:10px; box-shadow:0 2px 6px rgba(0,0,0,0.2); max-width:600px; display:block;">
</section>


        <section class="card" id="references">
            <h2>References</h2>
           <ol>
        <li>Cid et al., 2024 - Deep neural networks (X-Raydar) for multi-label chest x-ray abnormality detection. The Lancet Digital Health. <a href="https://www.thelancet.com/journals/landig/article/PIIS2589-7500(23)00218-2/fulltext" target="_blank">Link</a></li>
        <li>Akhter et al., 2023 - AI/ML review on chest X-rays: Structured review of CXR datasets, patents. Frontiers in Big Data. <a href="https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2023.1120989/full" target="_blank">Link</a></li>
        <li>MirÃ³ Catalina et al., 2024 - AI CAD platform (ChestEye) for multi-class chest x-ray abnormalities. Scientific Reports. <a href="https://www.nature.com/articles/s41598-024-55792-1" target="_blank">Link</a></li>
        <li>Geric et al., 2023 - Overview of AI CAD software for TB and thoracic diseases. PMC. <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC10171486/" target="_blank">Link</a></li>
        <li>Ã‡allÄ± et al., 2021 - Deep learning survey for chest X-ray analysis. Computer Vision and Image Understanding. <a href="https://www.sciencedirect.com/science/article/pii/S1361841521001717" target="_blank">Link</a></li>
        <li>Song et al., 2024 - AI chest X-ray enhancement techniques. AI in Medicine, Elsevier. <a href="https://www.sciencedirect.com/science/article/pii/S2666555724001205" target="_blank">Link</a></li>
        <li>Naz et al., 2023 - ResNet50 + LIME for explainable AI on TB, pneumonia, COVID-19. PMC. <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC9818469/" target="_blank">Link</a></li>
        <li>Anderson et al., 2024 - Deep learning CAD system improves physician accuracy. Scientific Reports. <a href="https://www.nature.com/articles/s41598-024-76608-2" target="_blank">Link</a></li>
        <li>Kufel et al., 2023 - DenseNet121 + Grad-CAM for multi-label classification of chest abnormalities. arXiv. <a href="https://arxiv.org/html/2202.03583v4" target="_blank">Link</a></li>
        <li>Mahamud et al., 2024 - DenseNet201 + SHAP, LIME multi-disease classification. AI in Healthcare, Elsevier. <a href="https://www.sciencedirect.com/science/article/pii/S2772662224001036" target="_blank">Link</a></li>
        <li>Fu et al., 2025 - LungMaxViT hybrid transformer for chest X-rays. Scientific Reports. <a href="https://www.nature.com/articles/s41598-025-90607-x" target="_blank">Link</a></li>
        <li>Shah et al., 2024 - Ensemble + transfer learning review of datasets. PMC. <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11446784/" target="_blank">Link</a></li>
        <li>Koyyada et al., 2023 - Weakly supervised CNN with localized disease identification. AI in Medicine, Elsevier. <a href="https://www.sciencedirect.com/science/article/pii/S2772442523000734" target="_blank">Link</a></li>
        <li>Bhusal & Panday, 2023 - DenseNet121 + Grad-CAM for thoracic disease prediction. arXiv. <a href="https://arxiv.org/html/2202.03583v4" target="_blank">Link</a></li>
        <li>Cervantes et al., 2024 - CNN transfer learning with DenseNet201 for multi-label accuracy. arXiv. <a href="https://arxiv.org/html/2505.16028v1" target="_blank">Link</a></li>
        <li>Ihongbe et al., 2024 - Review of explainable AI techniques for chest radiographs. PLOS ONE. <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0308758" target="_blank">Link</a></li>
        <li>Ong et al., 2021 - SqueezeNet + LIME, SHAP for multi-class COVID-19 detection. arXiv. <a href="https://arxiv.org/html/2505.16028v1" target="_blank">Link</a></li>
        <li>Huang et al., 2024 - Multi-disease CNN diagnosis using ChestX-ray14 dataset. PMC. <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC10963853/" target="_blank">Link</a></li>
        <li>Al-Adhaileh et al., 2025 - MobileNetV2 for institutional chest X-ray dataset. PMC. <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11926135/" target="_blank">Link</a></li>
        <li>Gasca Cervantes et al., 2024 - VGG16, DenseNet201, ResNet50 for chest X-ray classification. arXiv. <a href="https://arxiv.org/html/2505.16028v1" target="_blank">Link</a></li>
        <li>de Camargo et al., 2025 - Clinically validated AI algorithm for tuberculosis and CXR findings. Frontiers in Artificial Intelligence. <a href="https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2025.1512910/full" target="_blank">Link</a></li>
        <li>Konica Minolta R&D, 2025 - Deep learning + proprietary pre/post-processing (Finding-i). Konica Minolta Research. <a href="https://research.konicaminolta.com/en/report/2025/technical-papers/introduction-to-cxr-finding-i-an-ai-powered-chest-x-ray-diagnostic-support-system/" target="_blank">Link</a></li>
        <li>Bhave et al., 2024 - Deep learning ensemble for left ventricular abnormalities detection. European Heart Journal. <a href="https://academic.oup.com/eurheartj/article/45/22/2002/7617354" target="_blank">Link</a></li>
        <li>Monti et al., 2025 - AI system for Pneumothorax detection in chest X-rays. Neurocomputing. <a href="https://www.sciencedirect.com/science/article/pii/S0899707124002857" target="_blank">Link</a></li>
        <li>Wienholt et al., 2025 - MedicalPatchNet, patch-based self-explainable AI for CXR classification. arXiv. <a href="https://arxiv.org/abs/2509.07477" target="_blank">Link</a></li>
    </ol>
        </section>

    </div> <footer class="footer">
        <p>&copy; 2025 Raj Koyani - Project Documentation</p>
    </footer>

</body>
</html>